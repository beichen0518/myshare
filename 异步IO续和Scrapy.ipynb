{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异步I/O续和Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 死锁（deadlock）\n",
    "- 死锁的四个条件\n",
    "- 禁止抢占\n",
    "- 持有和等待\n",
    "- 互斥\n",
    "- 循环等待"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环引用问题\n",
    "- weakref 弱引用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import weakref\n",
    "class A():\n",
    "    pass\n",
    "    \n",
    "class B():\n",
    "    pass\n",
    "    \n",
    "a = A()\n",
    "b = B()\n",
    "# 在这里发生了循环调用，如果不做处理会发生严重的内存泄漏\n",
    "# 所以导入了weakref 让其中的一方弱引用，这样就不会算到引用计数里面，垃圾回收机制就会把它释放\n",
    "a.dept = b\n",
    "b.mgr = weakref.ref(a)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据套路\n",
    "- 几个比较重要的内置函数\n",
    "- 第一步：filter 过滤（筛选需要的数据） \n",
    "- 第二步：map 映射（把数据映射成想要的格式）\n",
    "- 第三歩：reduce(python 中 sum / max / min) 规约（提取出有用的信息）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.692130429902463\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "thy_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "print(sum(map(math.sqrt, filter(lambda x: x % 2 == 0, thy_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异步I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "def countdown(name, num):\n",
    "    while num > 0:\n",
    "        print(f'Countdown[{name}]: {num}')\n",
    "        # 异步执行 - 非阻塞\n",
    "        yield from asyncio.sleep(1)\n",
    "        # 同步执行 - 阻塞式的\n",
    "        # time.sleep(1)\n",
    "        num -= 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    # 异步I/O - 虽然只有一个线程但是两个任务相互之间不阻塞\n",
    "    tasks = [\n",
    "        countdown(\"A\", 10), countdown(\"B\", 5),\n",
    "    ]\n",
    "    loop.run_until_complete(asyncio.wait(tasks))\n",
    "    loop.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# pip install aiohttp\n",
    "# 用生成器请求网页的第三方库\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "@asyncio.coroutine\n",
    "async def download(url):\n",
    "    # 异步的方式取数据\n",
    "    print('[Fetch]:', url)\n",
    "    # 建立会话\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # 获取页面，与requests用法相似\n",
    "        async with session.get(url) as resp:\n",
    "            print(url, '--->', resp.status)\n",
    "            print(url, '--->', resp.cookies)\n",
    "            print('\\n\\n', await resp.text())\n",
    "\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    urls = [\n",
    "        'https://www.baidu.com',\n",
    "        'http://www.sohu.com',\n",
    "        'http://www.sina.com',\n",
    "        'https://www.taobao.com',\n",
    "        'http://www.qq.com'\n",
    "    ]\n",
    "    tasks = [download(url) for url in urls]\n",
    "    loop.run_until_complete(asyncio.wait(tasks))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy\n",
    "- 创建虚拟环境\n",
    "- python -m venv 虚拟环境名\n",
    "- 创建一个内置原来环境中已有第三方库的环境\n",
    "- python -m venv 虚拟环境名 --system-site-packages \n",
    "- python -m pip install -U pip\n",
    "- pip install scrapy\n",
    "    - 如果报错\n",
    "    - 需要安装这个依赖库Twisted-18.4.0-cp36-cp36m-win_amd64.whl\n",
    "    - pip install 文件路径+Twisted-18.4.0-cp36-cp36m-win_amd64.whl\n",
    "- 创建项目    \n",
    "- scrapy startproject 项目名 \n",
    "- 创建爬虫\n",
    "- scrapy genspider 爬虫名字 域名\n",
    "\n",
    "- scrapy shell url 用shell来试运行\n",
    "    - pip install pypiwin32\n",
    "- 启动\n",
    "- scrapy crawl 爬虫文件名\n",
    "- 将文件结果导入result.json\n",
    "- scrapy crawl 爬虫文件名 -o result.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- settings.py中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for douban project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://doc.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'douban'\n",
    "\n",
    "SPIDER_MODULES = ['douban.spiders']\n",
    "NEWSPIDER_MODULE = 'douban.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "# 修改请求头\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) ' \\\n",
    "             'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "# 是否遵守规则\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "# 并发请求数量\n",
    "CONCURRENT_REQUESTS = 2\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "# 下载延迟\n",
    "DOWNLOAD_DELAY = 5\n",
    "# The download delay setting will honor only one of:\n",
    "# CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "# CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "# 允许保存cookie\n",
    "COOKIES_ENABLED = True\n",
    "\n",
    "MONGODB_SERVER = '47.98.172.171'\n",
    "MONGODB_PORT = 27017\n",
    "MONGODB_DB = 'douban'\n",
    "MONGODB_COLLECTION = 'movie'\n",
    "\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "# TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "# DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "# }\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "# SPIDER_MIDDLEWARES = {\n",
    "#    'douban.middlewares.DoubanSpiderMiddleware': 543,\n",
    "# }\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "# 下载中间件\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "   'douban.middlewares.DoubanDownloaderMiddleware': 543,\n",
    "}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://doc.scrapy.org/en/latest/topics/extensions.html\n",
    "# EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "# }\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "# 让PIPELINES生效， 后面的数字表示执行的级别，越小越先执行\n",
    "ITEM_PIPELINES = {\n",
    "   'douban.pipelines.DoubanPipeline': 300,\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n",
    "# AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "# AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "# AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "# AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "# 缓存爬取过得页面，有助于提升爬虫性能\n",
    "HTTPCACHE_ENABLED = True\n",
    "HTTPCACHE_EXPIRATION_SECS = 0\n",
    "HTTPCACHE_DIR = 'httpcache'\n",
    "HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- movie.py中\n",
    "    - 这个文件是自己创建的爬虫文件在spiders文件下\n",
    "    - scrapy genspider movie movie.douban.com 是这个命令创建出来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "from douban.items import MovieItem\n",
    "\n",
    "\n",
    "class MovieSpider(scrapy.Spider):\n",
    "    name = 'movie'\n",
    "    allowed_domains = ['movie.douban.com']\n",
    "    start_urls = ['https://movie.douban.com/top250']\n",
    "\n",
    "    def parse(self, response):\n",
    "        li_list = response.xpath('//*[@id=\"content\"]/div/div[1]/ol/li')\n",
    "        for li in li_list:\n",
    "            item = MovieItem()\n",
    "            # 在后面加上text()表示去标签里面的内容\n",
    "            item['title'] = li.xpath('div/div[2]/div[1]/a/span[1]/text()').extract_first()\n",
    "            item['score'] = li.xpath('div/div[2]/div[2]/div/span[2]/text()').extract_first()\n",
    "            item['motto'] = li.xpath('div/div[2]/div[2]/p[2]/span/text()').extract_first()\n",
    "            yield item\n",
    "        # 用样式表拿\n",
    "        # a_list = response.css('a[href]::text') 取a标签里面的内容\n",
    "        # 取属性内容, .re可以加正则表达式\n",
    "        href_list = response.css('a[href]::attr(\"href\")').re('\\?start=.*')\n",
    "        for href in href_list:\n",
    "            # 将url补全\n",
    "            url = response.urljoin(href)\n",
    "            # 将新的url传给request, 并回调parse方法\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MovieItem(scrapy.Item):\n",
    "    \n",
    "    # 设置要提取出的字段\n",
    "    title = scrapy.Field()\n",
    "    score = scrapy.Field()\n",
    "    motto = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- middlewares.py\n",
    "    - 这里是设置了代理，防止封ip\n",
    "    - 并且是在download中间件下修改process_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XiciDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        # 设置代理\n",
    "        # proxyip放了一些代理ip,通过这种方法可以测试出哪个好用\n",
    "        # with open('E://proxyip.txt', 'r', encoding='utf-8') as f:\n",
    "        #     ip_list = f.readlines()\n",
    "        #     while True:\n",
    "        #         ip = choice(ip_list).strip()\n",
    "        #         if ip != 'ip:':\n",
    "        #             break\n",
    "        # 将可用的代理ip写入下面的列表中，每次执行文件随机选取\n",
    "        pro_list = [\n",
    "            'HTTPS://115.54.204.118:35868',\n",
    "            'HTTP://171.80.96.49:6666',\n",
    "            'HTTPS://218.75.114.6:63000',\n",
    "            'HTTPS://14.118.255.61:6666',\n",
    "            'HTTPS://223.145.230.84:6666',\n",
    "            'HTTP://175.155.24.51:808',\n",
    "            'HTTPS://183.143.91.251:40706',\n",
    "            'HTTPS://125.120.206.24:6666',\n",
    "            'HTTPS://123.171.43.142:808',\n",
    "        ]\n",
    "        request.meta['proxy'] = choice(pro_list)\n",
    "        # print(ip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pipelines.py中\n",
    "    - 持久化数据，需要在setting中将对应注释取消"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "import pymongo\n",
    "\n",
    "# 注意settings导入位置，不是在文件目录中的位置\n",
    "from scrapy.conf import settings\n",
    "\n",
    "class DoubanPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        connection = pymongo.MongoClient(settings['MONGODB_SERVER'], settings['MONGODB_PORT'])\n",
    "        db = connection[settings['MONGODB_DB']]\n",
    "        self.connection = db[settings['MONGODB_COLLECTION']]\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.connection.insert_one(dict(item))\n",
    "\n",
    "        return item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
