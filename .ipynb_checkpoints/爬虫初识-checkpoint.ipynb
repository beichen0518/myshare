{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络爬虫\n",
    "- 网络爬虫（web crawler），以前经常称之为网络蜘蛛（spider），是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本），曾经被广泛的应用于互联网搜索引擎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## robots\n",
    "- Robots协议，全称是“网络爬虫排除标准”\n",
    "- 根目录robots.txt\n",
    "- www.baidu.com/robots.txt\n",
    "\n",
    "## 查看网站规模\n",
    "- site:完整url 可以知道一个网站的规模\n",
    "- site:www.baidu.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 用户跟踪\n",
    "- cookie\n",
    "- url重写: 在url后加session id\n",
    "- 表单的隐藏域： type=hiden\n",
    "\n",
    "## cookie 和 session 关系\n",
    "- cookie 和session \n",
    "- cookie 放session id\n",
    "- 中间件提取出cookie中的session id ，添加到request中\n",
    "\n",
    "\n",
    "## cookie 类型\n",
    "- 浏览器续存期cookie: 浏览器不关 就不会消失 \n",
    "- 持久化cookie： 设置存活时间\n",
    "\n",
    "## http\n",
    "- HTTP请求（请求行+请求头+空行+[消息体]）\n",
    "\n",
    "- HTTP响应（响应行+响应头+空行+消息体）\n",
    "\n",
    "## 分析网页工具\n",
    "- POSTMAN 可以使用restful风格像后台请求数据\n",
    "- HTTPie 相当于命令行版的postman\n",
    "    - $ http --header http://www.scu.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BuiltWith：识别网站使用的技术\n",
    "    - 在python环境中\n",
    "```python\n",
    "import builtwith\n",
    "builtwith.parse('http://www.bootcss.com/')\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "builtwith.parse('https://www.jianshu.com/')\n",
    "```\n",
    "\n",
    "- python-whois：查询网站的所有者\n",
    "```python\n",
    "import whois\n",
    "whois.whois('www.baidu.com')\n",
    "```\n",
    "- robotparser：解析robots.txt的工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 页面解析\n",
    "- 正则表达式 - 性能好 但复杂\n",
    "- pyquery - jQuery - css选择器取元素\n",
    "    - #foo - <h1 id=\"foo\">\n",
    "    - .foo - <h1 class=\"foo\">\n",
    "    - p - <p>\n",
    "\n",
    "    - p h2 - <p><div><h2></h2></div></p>\n",
    "    - p>h2\n",
    "    - p~h2 - <p></p><div></div><h2></h2>\n",
    "    - p+h2 - <p></p><h2></h2>\n",
    "    \n",
    "    \n",
    "- BeautifulSoup - bs4\n",
    "- lxml - libxml2 - XPath 性能比前两个好 \n",
    "    - XML / JSON / YAML - 异构的系统之前的数据交换\n",
    "- pip install lxml bs4 \n",
    "- bs4 能够用lxml做解析器，性能更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "import pymysql\n",
    "\n",
    "\n",
    "def get_page_code(start_url, *, retry_times=3, charsets=('utf-8',)):  # 起始url, 尝试次数, 编码\n",
    "    # 起点， 可以在要爬取的网站随便选一个作为起点\n",
    "    try:\n",
    "        # 进行多种方式的解码\n",
    "        for charset in charsets:\n",
    "            try:\n",
    "                html = urlopen(start_url).read().decode(charset)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                html = None\n",
    "\n",
    "    except URLError as e:\n",
    "        print('Error:', e)\n",
    "        return get_page_code(start_url, retry_times=retry_times - 1, charsets=('utf-8', 'gbk', 'gb2312')) if retry_times > 0 else None\n",
    "    return html\n",
    "\n",
    "\n",
    "def main():\n",
    "    url_list = ['http://sports.sohu.com/nba_a.shtml']\n",
    "    # 为了防止重复爬取，要将访问过得页面加到集合中\n",
    "    # 集合可以自动去重\n",
    "    visited_list = set({})\n",
    "    while len(url_list) > 0:\n",
    "        current_url = url_list.pop(0)\n",
    "        visited_list.add(current_url)\n",
    "        html = get_page_code(current_url, charsets=('utf-8', 'gbk', 'gb2312'))\n",
    "        if html:\n",
    "            # 匹配正则表达式的内容， 并取()内的内容\n",
    "            link_regex = re.compile(r'<a[^>]+test=a\\s[^>]*href=[\"\\'](\\S*)[\"\\']', re.IGNORECASE)\n",
    "            link_list = re.findall(link_regex, html)\n",
    "            url_list += link_list\n",
    "            conn = pymysql.connect(host='localhost', port=3306,\n",
    "                                   db='crawler', user='root',\n",
    "                                   charset='utf8',\n",
    "                                   passwd='123456')\n",
    "\n",
    "        try:\n",
    "            for url in link_list:\n",
    "                if url not in visited_list:\n",
    "                    visited_list.add(url)\n",
    "                    print(url)\n",
    "                    ht = get_page_code(url, charsets=('utf-8', 'gbk', 'gb2312'))\n",
    "                    title_regex = re.compile(r'<h1>(.*)<span', re.IGNORECASE)\n",
    "                    #　title = re.findall(title_regex, ht)[0]\n",
    "                    match_list = title_regex.findall(ht)\n",
    "                    if len(match_list) > 0:\n",
    "                        title = match_list[0]\n",
    "                    with conn.cursor() as cursor:\n",
    "                        cursor.execute('insert into tb_result (rtitle, rurl) values (%s, %s)', (title, url))\n",
    "                        conn.commit()\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "        print('执行完成')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def main():\n",
    "    'http://sports.sohu.com/nba_a.shtml'\n",
    "    resp = requests.get('http://sports.sohu.com/nba_a.shtml')\n",
    "    html = resp.content.decode('gbk')\n",
    "    # 用lxml解析\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "    # bs.body.h1 拿到body下的h1\n",
    "    # bs.select('div p') 选择器\n",
    "    # bs.find_all(re.compile(r'^b')) 以b开头的标签\n",
    "    # bs.find(id='bar') 找标签, 找单个\n",
    "    # bs.find_all('p', {'class':'foo'}) 找到类为foo的p标签 返回一个列表\n",
    "    # bs.select('a[href]') 找到有href属性的a标签\n",
    "\n",
    "    # for elem in bs.select('a[href]'):\n",
    "    # 取到href中的内容\n",
    "    #     print(elem.attrs['href'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def main():\n",
    "    'http://sports.sohu.com/nba_a.shtml'\n",
    "    # 通过requests第三方库的get方法获取页面\n",
    "    resp = requests.get('http://sports.sohu.com/nba_a.shtml')\n",
    "    # 对响应的字符串（bytes）进行解码操作（搜狐的部分页面使用了gbk编码）\n",
    "    html = resp.content.decode('gbk')\n",
    "    # 用lxml解析 创建BeatulifulSoup对象来解析页面（相当于JavaScript的DOM）\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "    # 通过CSS选择器语法查找元素并通过循环进行处理\n",
    "    for elem in bs.select('a[test=a]'):\n",
    "        # 通过attrs属性（字典）获取元素的属性值\n",
    "        link_url = elem.attrs['href']\n",
    "        resp = requests.get(link_url)\n",
    "        bs_sub = BeautifulSoup(resp.text, 'lxml')\n",
    "        # .text 标签里面的内容\n",
    "        # attrs 标签的属性\n",
    "        # 将文本中的\\r\\n 替换成''\n",
    "        # 使用正则表达对获取的数据做进一步的处理\n",
    "        print(re.sub(r'[\\r\\n]', '', bs_sub.select_one('h1').text))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "作业：\n",
    "天行新闻\n",
    "\n",
    "知乎发现页\n",
    "www.zhihu.com/explore\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作业1 天行新闻链接打印\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_page_code(start_url, *, retry_time=3, charsets=('gb2312',)):\n",
    "    try:\n",
    "        for charset in charsets:\n",
    "            try:\n",
    "                resp = requests.get(start_url)\n",
    "                html = resp.content.decode(charset)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                html = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        return get_page_code(start_url, retry_time=retry_time - 1, charsets=charsets)\n",
    "    return html\n",
    "\n",
    "\n",
    "def main():\n",
    "    'http://news.sohu.com/20171226/n526348972.shtml'\n",
    "    \"\"\"<div class=\"list14\">\n",
    "    <ul>\n",
    "    <li>\n",
    "    <span>\n",
    "    17-09-10\n",
    "    </span>\n",
    "    <a href=\"http://news.17173.com/content/09102017/081739388_1.shtml\" onclick=\"return sogouRelateNews(this);\" len=\"6\"  ind=\"1\" target=\"_blank\">\n",
    "    综述:腾讯上线查小号功能,网吧下载H1Z1被公安查处\n",
    "    </a>\n",
    "    </li>\n",
    "    \"\"\"\n",
    "    url_list = ['http://news.sohu.com/20171226/n526348972.shtml']\n",
    "    visited_list = set()\n",
    "    while len(url_list) > 0:\n",
    "        current_url = url_list.pop(0)\n",
    "        print(current_url)\n",
    "        if current_url not in visited_list:\n",
    "            visited_list.add(current_url)\n",
    "            html = get_page_code(current_url, charsets=('gb2312', 'utf-8', 'gbk', 'ascii'))\n",
    "            if html:\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "                for elem in soup.select('div[class=\"mutu-news\"] a'):\n",
    "                    new_url = elem.attrs['href']\n",
    "                    if new_url not in visited_list and new_url not in url_list:\n",
    "                        url_list.append(new_url)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "作业2：爬取知乎的推荐内容\n",
    "data-za-element-name=\"Title\"\n",
    "url:https://www.zhihu.com/explore/recommendations\n",
    "    resp = requests.get(main_url,headers=headers)\n",
    "\n",
    "\n",
    "<h1 class=\"QuestionHeader-title\">\n",
    "如何评价 NBA 17-18 赛季西部决赛 G5 勇士 94:98 火箭，总比分火箭 3:2 领先？\n",
    "</h1>\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymysql\n",
    "\n",
    "\n",
    "def get_page_code(start_url, *, retry_time=3, charsets=('utf-8',)):\n",
    "    try:\n",
    "        for charset in charsets:\n",
    "            try:\n",
    "                headers = {\n",
    "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'\n",
    "                    }\n",
    "                resp = requests.get(start_url, headers=headers)\n",
    "\n",
    "                html = resp.content.decode(charset)\n",
    "                break\n",
    "            except UnicodeDecodeError as e:\n",
    "                html = None\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        return get_page_code(start_url, retry_time=retry_time - 1, charsets=charsets) if retry_time > 0 else None\n",
    "    return html\n",
    "\n",
    "\n",
    "def main():\n",
    "    url_list = []\n",
    "    start_url = 'https://www.zhihu.com/explore/recommendations'\n",
    "\n",
    "    html = get_page_code(start_url, charsets=('utf-8', 'gbk', 'gb2312'))\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    for elem in soup.select('a[class=\"question_link\"]'):\n",
    "        new_url = elem.attrs['href']\n",
    "        new_url = 'https://www.zhihu.com' + new_url\n",
    "        url_list.append(new_url)\n",
    "    while len(url_list) > 0:\n",
    "        visited_list = set()\n",
    "        current_url = url_list.pop(0)\n",
    "        if current_url not in visited_list:\n",
    "            visited_list.add(current_url)\n",
    "        html = get_page_code(current_url, charsets=('utf-8', 'gbk', 'gb2312'))\n",
    "\n",
    "        conn = pymysql.connect(host='localhost', port=3306,\n",
    "                               db='crawler', user='root',\n",
    "                               charset='utf8',\n",
    "                               passwd='123456')\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            elem = soup.select_one('h1[class=\"QuestionHeader-title\"]')\n",
    "            if elem:\n",
    "                title = elem.text\n",
    "\n",
    "                content = ''\n",
    "                for elem in soup.select('span[class=\"RichText ztext CopyrightRichText-richText\"] p'):\n",
    "                    content += elem.text\n",
    "\n",
    "                now_data = datetime.now()\n",
    "                with conn.cursor() as cursor:\n",
    "                    cursor.execute('insert into tb_questions (title, url, content, create_time) values (%s, %s, %s, %s)', (title, current_url, content, now_data))\n",
    "                    conn.commit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
