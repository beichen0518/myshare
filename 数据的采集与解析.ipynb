{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据的采集与解析\n",
    "- 下载数据 - urllib / requests / aiohttp。\n",
    "- 解析数据 - re / lxml / beautifulsoup4（bs4）/ pyquery。\n",
    "- 缓存和持久化 - pymysql / redis / sqlalchemy / peewee / pymongo。\n",
    "- 序列化和压缩 - pickle / json / zlib。\n",
    "- 调度器 - 进程 / 线程 / 协程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup的使用\n",
    "- find / find_all：字符串、正则表达式、列表、True、函数或Lambda。\n",
    "- select_one / select：CSS选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隔离级别\n",
    "- 由低到高\n",
    "- uncommitted read 读未提交， 性能最好，但会有脏数据（Dirty Read）\n",
    "- read committed - 提交后可读 不会有脏数据， 但会有不可重复读(Unrepeatable Read) \n",
    "- repeatable read - 可重复读数据 但会有幻读(Phantom Read)\n",
    "- serializable\n",
    "- 由低到高 隔离性越好，但性能越低\n",
    "- 查看数据库的隔离级别的命令\n",
    "- select @@tx_isolation;\n",
    "- 在全局修改事物隔离级别，这里设置为读级别\n",
    "- set global transaction isolation level read committed;\n",
    "- 临时修改隔离级别，这里设置的是最高级别\n",
    "- set session transaction isolation level serializable;\n",
    "- [为什么使用非关系性数据库](http://www.runoob.com/mongodb/nosql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 摘要\n",
    "- MD5 32个字节, SHA1 48个字节, SHA256 64个字节\n",
    "```python\n",
    "import hashlib\n",
    "# 创建一个md对象， 同样可以创建sha1 ,sha256对象\n",
    "hasher = hashlib.md5()\n",
    "link = 'hello'\n",
    "# 将link的内容变成字节再对其进行摘要\n",
    "hasher.update(link.encode('utf-8'))\n",
    "# 提取摘要结果\n",
    "hasher.hexdigest()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据储存\n",
    "- 序列化/反序列化\n",
    "- 序列化的三种工具 pickle / json / shelve\n",
    "- pickle : 可以将数据变成字节\n",
    "- json: dump /dumps / load / loads\n",
    "- 序列化-把对象变成字符或者字节序列\n",
    "- 反序列化-把字符或者字节序列还原成对象\n",
    "\n",
    "- 数据压缩\n",
    "- 需要先将数据序列化才可以进行数据压缩\n",
    "- zlib \n",
    "    - compress(data) 压缩数据\n",
    "    - decompress 解压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import re\n",
    "import pymysql\n",
    "import ssl\n",
    "\n",
    "from pip.utils import logging\n",
    "from pymysql import Error\n",
    "\n",
    "\n",
    "# 通过指定的字符集对页面进行解码(不是每个网站都将字符集设置为utf-8)\n",
    "def decode_page(page_bytes, charsets=('utf-8',)):\n",
    "    page_html = None\n",
    "    for charset in charsets:\n",
    "        try:\n",
    "            page_html = page_bytes.decode(charset)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "            # logging.error('Decode:', error)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "# 获取页面的HTML代码(通过递归实现指定次数的重试操作)\n",
    "def get_page_html(seed_url, *, retry_times=3, charsets=('utf-8',)):\n",
    "    page_html = None\n",
    "    try:\n",
    "        if seed_url.startswith('http://') or seed_url.startswith('https://'):\n",
    "            page_html = decode_page(urlopen(seed_url).read(), charsets)\n",
    "    except URLError:\n",
    "        # logging.error('URL:', error)\n",
    "        if retry_times > 0:\n",
    "            return get_page_html(seed_url, retry_times=retry_times - 1,\n",
    "                                 charsets=charsets)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "# 从页面中提取需要的部分(通常是链接也可以通过正则表达式进行指定)\n",
    "def get_matched_parts(page_html, pattern_str, pattern_ignore_case=re.I):\n",
    "    pattern_regex = re.compile(pattern_str, pattern_ignore_case)\n",
    "    return pattern_regex.findall(page_html) if page_html else []\n",
    "\n",
    "\n",
    "# 开始执行爬虫程序并对指定的数据进行持久化操作\n",
    "def start_crawl(seed_url, match_pattern, *, max_depth=-1):\n",
    "    conn = pymysql.connect(host='localhost', port=3306,\n",
    "                           database='crawler', user='root',\n",
    "                           password='123456', charset='utf8')\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            url_list = [seed_url]\n",
    "            visited_url_list = {seed_url: 0}\n",
    "            while url_list:\n",
    "                current_url = url_list.pop(0)\n",
    "                depth = visited_url_list[current_url]\n",
    "                if depth != max_depth:\n",
    "                    page_html = get_page_html(current_url, charsets=('utf-8', 'gbk', 'gb2312'))\n",
    "                    links_list = get_matched_parts(page_html, match_pattern)\n",
    "                    param_list = []\n",
    "                    for link in links_list:\n",
    "                        if link not in visited_url_list or link not in url_list:\n",
    "                            visited_url_list[link] = depth + 1\n",
    "                            page_html = get_page_html(link, charsets=('utf-8', 'gbk', 'gb2312'))\n",
    "                            # headings = get_matched_parts(page_html, r'<h1>(.*)<span')\n",
    "                            # if headings:\n",
    "                            #     param_list.append((headings[0], link))\n",
    "                            # 实例化md5\n",
    "                            hasher = hashlib.md5()\n",
    "                            # 将link的内容变成字节再对其进行摘要\n",
    "                            hasher.update(link.encode('utf-8'))\n",
    "                            # 将摘要的值和页面的元组加到列表中\n",
    "                            param_list.append((hasher.hexdigest(), page_html))\n",
    "                    # 批量持久化数据，列表中每有一个数据便执行一次        \n",
    "                    cursor.executemany('insert into tb_result values (default, %s, %s)',\n",
    "                                       param_list)\n",
    "                    conn.commit()\n",
    "    except Error as err:\n",
    "        logging.error('[SQL]:', err)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    start_crawl('http://sports.sohu.com/nba_a.shtml',\n",
    "                r'<a[^>]+href=[\"\\'](.*?)[\"\\']',\n",
    "                max_depth=2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests\n",
    "- 使用requests获取页面\n",
    "- GET请求和POST请求。\n",
    "- URL参数和请求头。\n",
    "- 复杂的POST请求（文件上传）。\n",
    "- 操作Cookie。\n",
    "- 设置代理服务器。\n",
    "- 超时设置。\n",
    "- 说明：关于requests的详细用法可以参考它的[官方文档](http://docs.python-requests.org/zh_CN/latest/user/quickstart.html)。\n",
    "- ccproxf可以让服务器做代理\n",
    "- [免费代理](http://www.xicidaili.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的User Agent\n",
    "\n",
    "1. Android\n",
    "\n",
    "    - Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19\n",
    "    - Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30\n",
    "    - Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1\n",
    "2. Firefox\n",
    "\n",
    "    - Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0\n",
    "    - Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0\n",
    "3. Google Chrome\n",
    "\n",
    "    - Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36\n",
    "    - Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19\n",
    "4. iOS\n",
    "\n",
    "    - Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3\n",
    "    - Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    headers = {'user-agent': 'Baiduspider'}  # 可以冒充百度的爬虫，也可以冒充是浏览器\n",
    "    proxies = {\n",
    "        'http': '61.135.217.7:80'\n",
    "    }  # 设置代理，这样可以隐藏自己的ip\n",
    "    base_url = 'https://www.zhihu.com/'\n",
    "    seed_url = urljoin(base_url, 'explore')  # 可以自动比较url 并补全\n",
    "    resp = requests.get(seed_url,\n",
    "                        headers=headers,\n",
    "                        proxies=proxies)\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    href_regex = re.compile(r'^/question')\n",
    "    link_set = set()\n",
    "    for a_tag in soup.find_all('a', {'href': href_regex}):\n",
    "        if 'href' in a_tag.attrs:\n",
    "            href = a_tag.attrs['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            link_set.add(full_url)\n",
    "    print(link_set)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mysql \n",
    "- blob 二进制大对象 -longblob\n",
    "- clob 字符大对象 - longtext\n",
    "- char 表示定长\n",
    "- 单列可存4G\n",
    "- fastDFS 最好的存放静态资源\n",
    "```\n",
    "create table tb_result(\n",
    "resultid integer not null auto_increment,\n",
    "rdigest char(32) not null,\n",
    "rpage longtext not null,\n",
    "rdate timestamp default now(),\n",
    "primary key(resultid)\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Redis两种持久化方案\n",
    "    1. RDB 对数据进行储存\n",
    "    2. AOF 优先使用 对步骤进行存储\n",
    "- [redis中文命令网](http://redisdoc.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import re\n",
    "import redis\n",
    "import ssl\n",
    "import hashlib\n",
    "import logging\n",
    "import pickle\n",
    "import zlib\n",
    "\n",
    "# Redis有两种持久化方案\n",
    "# 1. RDB\n",
    "# 2. AOF\n",
    "\n",
    "\n",
    "# 通过指定的字符集对页面进行解码(不是每个网站都将字符集设置为utf-8)\n",
    "def decode_page(page_bytes, charsets=('utf-8',)):\n",
    "    page_html = None\n",
    "    for charset in charsets:\n",
    "        try:\n",
    "            page_html = page_bytes.decode(charset)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "            # logging.error('[Decode]', err)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "# 获取页面的HTML代码(通过递归实现指定次数的重试操作)\n",
    "def get_page_html(seed_url, *, retry_times=3, charsets=('utf-8',)):\n",
    "    page_html = None\n",
    "    try:\n",
    "        if seed_url.startswith('http://') or \\\n",
    "                seed_url.startswith('https://'):\n",
    "            page_html = decode_page(urlopen(seed_url).read(), charsets)\n",
    "    except URLError as err:\n",
    "        logging.error('[URL]', err)\n",
    "        if retry_times > 0:\n",
    "            return get_page_html(seed_url, retry_times=retry_times - 1,\n",
    "                                 charsets=charsets)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "# 从页面中提取需要的部分(通常是链接也可以通过正则表达式进行指定)\n",
    "def get_matched_parts(page_html, pattern_str, pattern_ignore_case=re.I):\n",
    "    pattern_regex = re.compile(pattern_str, pattern_ignore_case)\n",
    "    return pattern_regex.findall(page_html) if page_html else []\n",
    "\n",
    "\n",
    "# 开始执行爬虫程序并对指定的数据进行持久化操作\n",
    "def start_crawl(seed_url, match_pattern, *, max_depth=-1):\n",
    "    # 创建redis客户端\n",
    "    client = redis.Redis(host='', port=, password='')\n",
    "    charsets = ('utf-8', 'gbk', 'gb2312')\n",
    "    logging.info('[Redis ping]', client.ping())\n",
    "    url_list = [seed_url]\n",
    "    visited_url_list = {seed_url: 0}\n",
    "    while url_list:\n",
    "        current_url = url_list.pop(0)\n",
    "        depth = visited_url_list[current_url]\n",
    "        if depth != max_depth:\n",
    "            page_html = get_page_html(current_url, charsets=charsets)\n",
    "            links_list = get_matched_parts(page_html, match_pattern)\n",
    "            for link in links_list:\n",
    "                if link not in visited_url_list:\n",
    "                    visited_url_list[link] = depth + 1\n",
    "                    page_html = get_page_html(link, charsets=charsets)\n",
    "                    if page_html:\n",
    "                        hasher = hashlib.md5()\n",
    "                        hasher.update(link.encode('utf-8'))\n",
    "                        # pickle.dumps 将数据序列化\n",
    "                        # zlib.compress 将数据压缩\n",
    "                        zipped_page = zlib.compress(pickle.dumps(page_html))\n",
    "                        # 将压缩好的数据存到redis中\n",
    "                        client.set(hasher.hexdigest(), zipped_page)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    start_crawl('http://sports.sohu.com/nba_a.shtml',\n",
    "                r'<a[^>]+test=a\\s[^>]*href=[\"\\'](.*?)[\"\\']',\n",
    "                max_depth=2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import zlib\n",
    "import pickle\n",
    "\n",
    "\n",
    "def main():\n",
    "    client = redis.Redis(host='', port=, password='')\n",
    "    # 从redis取到对应的压缩的页面\n",
    "    html = client.get('67f568c2f8d9e621c4170bc7fa56ba18')\n",
    "    # 解压缩\n",
    "    html = zlib.decompress(html)\n",
    "    # 反序列化\n",
    "    html = pickle.loads(html)\n",
    "    print(html)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
