{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫并发下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存在内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入枚举\n",
    "from enum import Enum, unique\n",
    "from queue import Queue\n",
    "from random import random\n",
    "from time import sleep\n",
    "from threading import Thread, current_thread\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def decode_page(page_bytes, charsets=('utf-8',)):\n",
    "    page_html = None\n",
    "    for charset in charsets:\n",
    "        try:\n",
    "            page_html = page_bytes.decode(charset)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "            # logging.error('[Decode]', err)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "class Retry(object):\n",
    "\n",
    "    def __init__(self, *, retry_times=3, wait_secs=1, errors=(Exception, )):\n",
    "        \"\"\"\n",
    "\n",
    "        :param retry_times: 重试次数\n",
    "        :param wait_secs: 最少等待时间\n",
    "        :param errors: 抓取的错误列表\n",
    "        \"\"\"\n",
    "        self.retry_times = retry_times\n",
    "        self.wait_secs = wait_secs\n",
    "        self.errors = errors\n",
    "\n",
    "    # 可以将对象变成像函数一样可以调用\n",
    "    def __call__(self, fn):\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for _ in range(self.retry_times):\n",
    "                try:\n",
    "                    return fn(*args, **kwargs)\n",
    "                # except后面可以跟元组\n",
    "                except self.errors as e:\n",
    "                    # logging.error(e)\n",
    "                    # logging.info('[Retry]')\n",
    "                    print(e)\n",
    "                    sleep((random() + 1) * self.wait_secs)\n",
    "\n",
    "            return None\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "#  枚举的作用是定义常量，而且可以提供一些方法来对这些常量验证\n",
    "#  保证里面没有重复值\n",
    "@unique\n",
    "class SpiderStatus(Enum):\n",
    "    IDLE = 0\n",
    "    WORKING = 1\n",
    "\n",
    "\n",
    "class Spider(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.status = SpiderStatus.IDLE\n",
    "\n",
    "    # 重试的包装器\n",
    "    @Retry()\n",
    "    def fetch(self, current_url, *, charsets=('utf-8', ), user_agent=None, proxies=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param current_url: 传入一个url\n",
    "        :return: 一个html页面\n",
    "        \"\"\"\n",
    "        thread_name = current_thread().name\n",
    "        print(f'[{thread_name} Fetch]:{current_url}')\n",
    "        headers = {'user-agent': user_agent} if user_agent else {}\n",
    "        resp = requests.get(current_url,\n",
    "                            headers=headers, proxies=proxies)\n",
    "\n",
    "        return decode_page(resp.content, charsets) if resp.status_code == 200 else None\n",
    "\n",
    "    def parse(self, html_page, *, domain='m.sohu.com'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param html_page: 传入一个html页面\n",
    "        :return:返回一个新的url列表\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_page, 'lxml')\n",
    "        url_links= []\n",
    "        for a_tag in soup.body.select('a[href]'):\n",
    "            # 解析url, 可以将url分解\n",
    "            parser = urlparse(a_tag.attrs['href'])\n",
    "            # 提取域名\n",
    "            netloc = parser.netloc or domain\n",
    "            # 提取协议\n",
    "            scheme = parser.scheme or 'http'\n",
    "            if scheme != 'javascript' and netloc == domain:\n",
    "                # 提取路由\n",
    "                path = parser.path\n",
    "                # 提取参数\n",
    "                query = '?' + parser.query if parser.query else ''\n",
    "                # 新的格式化字符串的方式，可以直接将变量传进去\n",
    "                full_url = f'{scheme}://{netloc}{path}{query}'\n",
    "                if full_url not in visited_urls:\n",
    "                    url_links.append(full_url)\n",
    "\n",
    "        return url_links\n",
    "\n",
    "    def extract(self, html_page):\n",
    "        \"\"\"\n",
    "\n",
    "        :param html_page: 传入一个html页面\n",
    "        :return: 返回提取到的数据\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def store(self, data_dict):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data_dict: 传入数据\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SpiderThread(Thread):\n",
    "\n",
    "    def __init__(self, name, spider, tasks_queue):\n",
    "        \"\"\"\n",
    "\n",
    "        :param spider: 爬虫\n",
    "        :param tasks_queue: 需要执行的任务\n",
    "        \"\"\"\n",
    "        super().__init__(name=name, daemon=True)\n",
    "        self.spider = spider\n",
    "        self.tasks_queue = tasks_queue\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            # 如果队列没东西时，会阻塞在这里，而且可以设置等待时间\n",
    "            current_url = self.tasks_queue.get()\n",
    "            visited_urls.add(current_url)\n",
    "            self.spider.status = SpiderStatus.WORKING\n",
    "            # 抓取页面\n",
    "            html_page = self.spider.fetch(current_url)\n",
    "            if html_page not in [None, '']:\n",
    "                # 解析页面，提取其中的url\n",
    "                url_links = self.spider.parse(html_page)\n",
    "                for url_link in url_links:\n",
    "                    self.tasks_queue.put(url_link)\n",
    "\n",
    "            self.spider.status = SpiderStatus.IDLE\n",
    "\n",
    "\n",
    "def is_any_alive(spider_threads):\n",
    "    \"\"\"\n",
    "\n",
    "    :param spider_threads: 传入所有线程\n",
    "    :return: 如果所有工作都完成，返回false, 如果有一个没结束，返回True\n",
    "    \"\"\"\n",
    "    # 只要有任何一个结果是true 结果就是true 与all正好相反\n",
    "    return any([spider_thread.spider.status == SpiderStatus.WORKING\n",
    "                for spider_thread in spider_threads])\n",
    "\n",
    "visited_urls = set()\n",
    "\n",
    "\n",
    "def main():\n",
    "    #  用队列是因为队列有锁，保证线程安全，\n",
    "    #  而且可以设置最大长度，可以控制内存的\n",
    "    #  先进先出，后进后出\n",
    "    task_queue = Queue()\n",
    "    # 将种子url放进队列， 并放在尾部\n",
    "    task_queue.put('http://m.sohu.com/')\n",
    "    #  创建一个线程队列\n",
    "    spider_threads = [SpiderThread('t%d' % i, Spider(), task_queue) for i in range(10)]\n",
    "    #  启动线程\n",
    "    for spider_thread in spider_threads:\n",
    "        spider_thread.start()\n",
    "    #  如果队列内不为空， 或者还有任务没完成\n",
    "    while not task_queue.empty() or is_any_alive(spider_threads):\n",
    "        pass\n",
    "\n",
    "    print('Over!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存在redis和mongodb中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入枚举\n",
    "from enum import Enum, unique\n",
    "from hashlib import sha1\n",
    "from random import random\n",
    "from time import sleep\n",
    "from threading import Thread, current_thread\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pymongo\n",
    "import requests\n",
    "import redis\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def decode_page(page_bytes, charsets=('utf-8',)):\n",
    "    page_html = None\n",
    "    for charset in charsets:\n",
    "        try:\n",
    "            page_html = page_bytes.decode(charset)\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "            # logging.error('[Decode]', err)\n",
    "    return page_html\n",
    "\n",
    "\n",
    "class Retry(object):\n",
    "\n",
    "    def __init__(self, *, retry_times=3, wait_secs=1, errors=(Exception, )):\n",
    "        \"\"\"\n",
    "\n",
    "        :param retry_times: 重试次数\n",
    "        :param wait_secs: 最少等待时间\n",
    "        :param errors: 抓取的错误列表\n",
    "        \"\"\"\n",
    "        self.retry_times = retry_times\n",
    "        self.wait_secs = wait_secs\n",
    "        self.errors = errors\n",
    "\n",
    "    # 可以将对象变成像函数一样可以调用\n",
    "    def __call__(self, fn):\n",
    "\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for _ in range(self.retry_times):\n",
    "                try:\n",
    "                    return fn(*args, **kwargs)\n",
    "                # except后面可以跟元组\n",
    "                except self.errors as e:\n",
    "                    # logging.error(e)\n",
    "                    # logging.info('[Retry]')\n",
    "                    print(e)\n",
    "                    sleep((random() + 1) * self.wait_secs)\n",
    "\n",
    "            return None\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "#  枚举的作用是定义常量，而且可以提供一些方法来对这些常量验证\n",
    "#  保证里面没有重复值\n",
    "@unique\n",
    "class SpiderStatus(Enum):\n",
    "    IDLE = 0\n",
    "    WORKING = 1\n",
    "\n",
    "\n",
    "class Spider(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.status = SpiderStatus.IDLE\n",
    "\n",
    "    # 重试的包装器\n",
    "    @Retry()\n",
    "    def fetch(self, current_url, *, charsets=('utf-8', ), user_agent=None, proxies=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param current_url: 传入一个url\n",
    "        :return: 一个html页面\n",
    "        \"\"\"\n",
    "        thread_name = current_thread().name\n",
    "        print(f'[{thread_name} Fetch]:{current_url}')\n",
    "        headers = {'user-agent': user_agent} if user_agent else {}\n",
    "        resp = requests.get(current_url,\n",
    "                            headers=headers, proxies=proxies)\n",
    "\n",
    "        return decode_page(resp.content, charsets) if resp.status_code == 200 else None\n",
    "\n",
    "    def parse(self, html_page, *, domain='m.sohu.com'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param html_page: 传入一个html页面\n",
    "        :return:返回一个新的url列表\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_page, 'lxml')\n",
    "        url_links= []\n",
    "        for a_tag in soup.body.select('a[href]'):\n",
    "            # 解析url, 可以将url分解\n",
    "            parser = urlparse(a_tag.attrs['href'])\n",
    "            # 提取域名\n",
    "            netloc = parser.netloc or domain\n",
    "            # 提取协议\n",
    "            scheme = parser.scheme or 'http'\n",
    "            if scheme != 'javascript' and netloc == domain:\n",
    "                # 提取路由\n",
    "                path = parser.path\n",
    "                # 提取参数\n",
    "                query = '?' + parser.query if parser.query else ''\n",
    "                # 新的格式化字符串的方式，可以直接将变量传进去\n",
    "                full_url = f'{scheme}://{netloc}{path}{query}'\n",
    "                if not redis_client.sismember('visited_urls', full_url) :\n",
    "                    redis_client.rpush('m_sohu_task', full_url)\n",
    "\n",
    "    def extract(self, html_page):\n",
    "        \"\"\"\n",
    "\n",
    "        :param html_page: 传入一个html页面\n",
    "        :return: 返回提取到的数据\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def store(self, data_dict):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data_dict: 传入数据\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SpiderThread(Thread):\n",
    "\n",
    "    def __init__(self, name, spider):\n",
    "        \"\"\"\n",
    "\n",
    "        :param spider: 爬虫\n",
    "        :param tasks_queue: 需要执行的任务\n",
    "        \"\"\"\n",
    "        super().__init__(name=name, daemon=True)\n",
    "        self.spider = spider\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "\n",
    "            # 如果队列没东西时，会阻塞在这里，而且可以设置等待时间\n",
    "            current_url = redis_client.lpop('m_sohu_task')\n",
    "            # 抓取不到继续抓取直到抓到为止\n",
    "            while not current_url:\n",
    "                current_url = redis_client.lpop('m_sohu_task')\n",
    "            self.spider.status = SpiderStatus.WORKING\n",
    "            current_url = current_url.decode('utf-8')\n",
    "            if not redis_client.sismember('visited_urls', current_url):\n",
    "                redis_client.sadd('visited_urls', current_url)\n",
    "                # 抓取页面\n",
    "                html_page = self.spider.fetch(current_url)\n",
    "                if html_page not in [None, '']:\n",
    "                    hasher = hasher_proto.copy()\n",
    "                    hasher.update(current_url.encode('utf-8'))\n",
    "                    doc_id = hasher.hexdigest()\n",
    "                    #  find 和 findone 的区别\n",
    "                    # find如果找不到数据，返回一个游标\n",
    "                    # findone如果找不到数据，返回False\n",
    "                    if sohu_data_coll.find({'_id': doc_id}).count() == 0:\n",
    "                        print(sohu_data_coll)\n",
    "                        sohu_data_coll.insert_one(\n",
    "                            {'_id':doc_id,\n",
    "                             'url':current_url,\n",
    "                             'page': html_page})\n",
    "\n",
    "                    # 如果往mongo中放二进制数据,需要用下面的方法包装\n",
    "                    # from bson import Binary\n",
    "                    # bson这个包千万不要自己下，而是要下mongo时自带下载，不然mongo操作中会包莫名错误\n",
    "                    # Binary()\n",
    "                    # 解析页面，提取其中的url\n",
    "                    self.spider.parse(html_page)\n",
    "\n",
    "                self.spider.status = SpiderStatus.IDLE\n",
    "\n",
    "\n",
    "def is_any_alive(spider_threads):\n",
    "    \"\"\"\n",
    "\n",
    "    :param spider_threads: 传入所有线程\n",
    "    :return: 如果所有工作都完成，返回false, 如果有一个没结束，返回True\n",
    "    \"\"\"\n",
    "    # 只要有任何一个结果是true 结果就是true 与all正好相反\n",
    "    return any([spider_thread.spider.status == SpiderStatus.WORKING\n",
    "                for spider_thread in spider_threads])\n",
    "\n",
    "\n",
    "redis_client = redis.Redis(host='47.98.172.171',\n",
    "                               port=11223, password='544619417wxz')\n",
    "\n",
    "mongo_client = pymongo.MongoClient(host='47.98.172.171', port=27017)\n",
    "db = mongo_client.msohu\n",
    "sohu_data_coll = db.webpages\n",
    "hasher_proto = sha1()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(mongo_client)\n",
    "    # 判断redis是否有m_sohu_task这个键\n",
    "    if not redis_client.exists('m_sohu_task'):\n",
    "        # 将种子url放进队列， 并放在尾部\n",
    "        redis_client.rpush('m_sohu_task', 'http://m.sohu.com/')\n",
    "    else:\n",
    "        pass\n",
    "    #  创建一个线程队列\n",
    "    # redis本身是线程安全的是，单线程异步io模式\n",
    "    spider_threads = [SpiderThread('t%d' % i, Spider()) for i in range(10)]\n",
    "    #  启动线程\n",
    "    for spider_thread in spider_threads:\n",
    "        spider_thread.start()\n",
    "    #  如果redis中m_sohu_task长度是否为0， 或者还有任务没完成\n",
    "    while redis_client.exists('m_sohu_task') or is_any_alive(spider_threads):\n",
    "        pass\n",
    "\n",
    "    print('Over!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
